---
title: "Exercise week 4"
output: html_document
---

```{r setup, include=FALSE}
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
```

## Load the Boston data from MASS package

```{r echo=TRUE}
library(MASS)
data("Boston")
dim(Boston)
str(Boston)
```
*The Boston data frame has 506 rows and 14 columns. The variables are listed below:*

*crim: per capita crime rate by town.*

*zn: proportion of residential land zoned for lots over 25,000 sq.ft.*

*indus: proportion of non-retail business acres per town.*

*chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).*

*nox: nitrogen oxides concentration (parts per 10 million).*

*rm: average number of rooms per dwelling.*

*age: proportion of owner-occupied units built prior to 1940.*

*dis: weighted mean of distances to five Boston employment centres.*

*rad: index of accessibility to radial highways.*

*tax :full-value property-tax rate per \$10,000.*

*ptratio: pupil-teacher ratio by town.*

*black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.*

*lstat: lower status of the population (percent).*

*medv: median value of owner-occupied homes in \$1000s.*

## Show a graphical overview of the data and show summaries of the variables in the data.
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
install.packages("dplyr", repos = "https://ftp.acc.umu.se/mirror/CRAN/")
library(dplyr)
library(tidyr)
library(ggplot2)
install.packages("corrplot")
library(corrplot)
library(GGally)
```

```{r echo=TRUE}
summary(Boston)
```
```{r echo=TRUE}
pairs(Boston)
```
### the correlation matrix
```{r echo=TRUE}
cor_matrix<-cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```
*The correlation matrix can be used to analyse the relationship between the variables. The function "corrplot" helps us to visualize  the said relationship: stronger correlation leads to bigger circles and more intense colours. The blue coulour indicates negative correlation and the red colour positive correlation. In this case the strongest negative correlation seems to be between the variables "nox" and "dis" and the strongest positive correlation between the variables "rad" and "tax".*

## Standardize the dataset

```{r echo=TRUE}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
```
*The data is now scaled and the mean of all of the variables is 0.*

# Create a categorical variable of the crime rate
```{r echo=TRUE}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# drop the old crime rate variable from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

## Divide the dataset to train and test sets
```{r echo=TRUE}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

## Fit the linear discriminant analysis on the train set
```{r echo=TRUE}
lda.fit <- lda(crime ~ ., data = train)
lda.fit
```
### LDA biplot
```{r echo=TRUE}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

## Save the crime categories from the test set and remove the categorical crime variable from the test dataset
```{r echo=TRUE}
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
```
## Predict the classes with the LDA model and cross tabulate the results
```{r echo=TRUE}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```
*Especially the prediction of high class was successful.*

## Reload the Boston dataset and standardize the dataset
```{r echo=TRUE}
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
```
## Calculate the distance between the observations
```{r echo=TRUE}
dist_eu <- dist(boston_scaled)
summary(dist_eu)
```
## Run k-means algorithm on the dataset
```{r echo=TRUE}
km <-kmeans(boston_scaled, centers = 3)
pairs(boston_scaled, col = km$cluster)
```
## Investigate the optimal number of clusters
```{r echo=TRUE}
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
*The optimal number of clusters is when the total WCSS drops radically, so here it is 2.*
```{r echo=TRUE}
km <- kmeans(boston_scaled, center = 2)
pairs(boston_scaled, col = km$cluster)
```
